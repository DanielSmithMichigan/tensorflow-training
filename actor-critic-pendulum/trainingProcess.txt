# Get gradient from critic. Output of critic to input action.
self.action_grads = tf.gradients(self.out, self.action)

# Input gradient to actor network
self.sess.run(self.optimize, feed_dict={
    self.inputs: inputs,
    self.action_gradient: a_gradient
})


# Store gradient in placeholder
self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])

#Get combined gradient. All parameters to output, multiplied by action gradient
self.unnormalized_actor_gradients = tf.gradients(self.scaled_out, self.network_params, -self.action_gradient)

# Divide actor gradients by batch size
self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))

# Optimization Op
self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(self.actor_gradients, self.network_params))